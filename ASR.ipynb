{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Dataset for colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QR3bn2n1iuNQ"
   },
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile(\"wav.zip\", \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"/content/audio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlV6_upwiwKF"
   },
   "outputs": [],
   "source": [
    "# with zipfile.ZipFile(\"original_txt.zip\", \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"/content/transcript\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PDNrHSHLy5_d"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import zipfile\n",
    "import threading\n",
    "\n",
    "import fitz\n",
    "import torch\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import soundfile as sf\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "from typing import List, Tuple, Dict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torchaudio.set_audio_backend(\"sox_io\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTokenizer:\n",
    "    \"\"\"\n",
    "    Base tokenizer which is inherited by different tokenizers.\n",
    "    This class has all static methods and utils for a tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    SPECIAL_TOKENS = {\n",
    "        \"pad\": \"<PAD>\",\n",
    "        \"unk\": \"<UNK>\",\n",
    "        \"bos\": \"<BOS>\",\n",
    "        \"eos\": \"<EOS>\",\n",
    "        \"book_name\": \"<book_name>\",\n",
    "        \"seperator\": \"</W>\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, config, **kwargs):\n",
    "        self.config = config\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.max_merges = config.max_merges\n",
    "\n",
    "        self.special_tokens = self.SPECIAL_TOKENS\n",
    "        self.vocab = {}         # Actual vocabulary of the tokenizer\n",
    "        self.inv_vocab = {}     # Inverse of vocabulary, used for decoding\n",
    "        self.merges = []        # List of all merges made during vocabulary building\n",
    "        self.metadata = {       # Tracks tokenizer state for checkpointing\n",
    "            \"merge_count\": 0,\n",
    "            \"books_used\": None,\n",
    "        }\n",
    "        self.padding_token = None\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Saves the current state of the tokenizer to the given filepath\"\"\"\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"metadata\": self.metadata,\n",
    "                \"vocab\": self.vocab,\n",
    "                \"merges\": self.merges,\n",
    "                \"special_tokens\": self.special_tokens,\n",
    "            }, f, indent=2)\n",
    "\n",
    "    def load(self, filepath: str):\n",
    "        \"\"\"Loads the tokenizer from a saved file\"\"\"\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            self.vocab = data[\"vocab\"]\n",
    "            self.merges = [tuple(pair) for pair in data[\"merges\"]]\n",
    "            self.special_tokens = data[\"special_tokens\"]\n",
    "            self.metadata = data.get(\"metadata\", {})\n",
    "            self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> List[str]:\n",
    "        \"\"\"Decodes a list or tensor of token IDs back to string\"\"\"\n",
    "        if isinstance(token_ids, torch.Tensor):\n",
    "            token_ids = token_ids.tolist()\n",
    "\n",
    "        words = []\n",
    "        for idx in token_ids:\n",
    "            token = self.inv_vocab.get(idx, self.special_tokens[\"unk\"])\n",
    "            if token == self.special_tokens[\"seperator\"]:\n",
    "                continue\n",
    "            words.append(token)\n",
    "        decoded = \"\".join(words).strip()\n",
    "        return self.remove_special_tokens(decoded)\n",
    "\n",
    "    def remove_special_tokens(self, decoded_output: str) -> str:\n",
    "        \"\"\"Removes special tokens and cleans up decoded text.\"\"\"\n",
    "        return (\n",
    "            decoded_output\n",
    "            .replace(self.special_tokens[\"bos\"], \"\")\n",
    "            .replace(self.special_tokens[\"eos\"], \"\")\n",
    "            .replace(self.special_tokens[\"pad\"], \"\")\n",
    "            .replace(self.special_tokens[\"unk\"], \"\")\n",
    "            .replace(self.special_tokens[\"seperator\"], \" \")\n",
    "            .strip()\n",
    "        )\n",
    "\n",
    "    def add_special_tokens(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Adds BOS and EOS tokens, and inserts `</W>` separator between all tokens including special tokens.\n",
    "        \"\"\"\n",
    "        return f\"{self.special_tokens['bos']} {text} {self.special_tokens['eos']}\"\n",
    "\n",
    "\n",
    "    def pad(self, token_ids: List[int]) -> List[int]:\n",
    "        \"\"\"Pads a list of token IDs to max_length using the pad token\"\"\"\n",
    "        pad_id = self.vocab.get(self.special_tokens[\"pad\"], 0)\n",
    "        return token_ids + [pad_id] * (config.n_ctx - len(token_ids)) if len(token_ids) < config.n_ctx else token_ids\n",
    "\n",
    "    def truncate(self, token_ids: List[int], max_length: int) -> List[int]:\n",
    "        \"\"\"Truncates a list of token IDs to max_length\"\"\"\n",
    "        return token_ids[:max_length]\n",
    "\n",
    "    def get_vocab_size(self) -> int:\n",
    "        \"\"\"Returns the size of the vocabulary\"\"\"\n",
    "        return len(self.vocab)\n",
    "\n",
    "    def set_padding_token(self, tokenID: int):\n",
    "        \"\"\"Sets the token ID used for padding\"\"\"\n",
    "        self.padding_token = tokenID\n",
    "\n",
    "    def encode_tensor(self, text) -> torch.Tensor:\n",
    "        return torch.tensor(self.encode(text), dtype=torch.long)\n",
    "\n",
    "    def _preprocess_text_to_corpus(self, text: str) -> Counter:\n",
    "        \"\"\"\n",
    "        Converts raw text into a Counter of space-separated character tokens\n",
    "        with </w> to indicate word boundaries. Used by BPE tokenizers.\n",
    "        Args:\n",
    "            text (str): Raw input text string\n",
    "        Returns:\n",
    "            Counter: frequency map of pre-tokenized words\n",
    "        \"\"\"\n",
    "        words = text.strip().split()\n",
    "        return Counter([\" \".join(tuple(word)) + SPECIAL_TOKENS[\"seperator\"] for word in words])\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_text_from_corpus(corpus, book_names, max_books, max_characters):\n",
    "        \"\"\"Combines text from cleaned processed extracted text to be used during tokenizer training\"\"\"\n",
    "        \n",
    "        if book_names is not None and max_books is not None:\n",
    "            raise Exception(\"Provide only one of `book_names` or `max_books`, not both.\")\n",
    "    \n",
    "        char_accum = 0\n",
    "        book_count = 0\n",
    "        combined_text = \"\"\n",
    "        books_used = {}\n",
    "    \n",
    "        for book, content in clean_text.items():\n",
    "            if book_names is not None and book not in book_names:\n",
    "                continue\n",
    "            if max_books is not None and book_count >= max_books:\n",
    "                break\n",
    "            if max_characters is not None and char_accum >= max_characters:\n",
    "                break\n",
    "            if max_characters is not None:\n",
    "                remaining = max_characters - char_accum\n",
    "                if remaining <= 0:\n",
    "                    break\n",
    "                book_slice = content[:remaining]\n",
    "            else:\n",
    "                book_slice = content\n",
    "\n",
    "            combined_text += book_slice\n",
    "            char_accum += len(book_slice)\n",
    "            books_used[book] = len(book_slice)\n",
    "            book_count += 1\n",
    "        return combined_text, books_used, char_accum, book_count\n",
    "    \n",
    "    def base_build_vocab(self,\n",
    "                         corpus: dict, \n",
    "                         folder_path: str,\n",
    "                         book_names: list = None,\n",
    "                         max_books: int = None,\n",
    "                         max_characters: int = None,\n",
    "                         checkpoint_interval: int = 5000,\n",
    "                        ):\n",
    "        \"\"\"\n",
    "        Trains tokenizer on a subset of books with flexible stopping conditions.\n",
    "        Args:\n",
    "            corpus (dict): Dictionary of book_name -> book_content.\n",
    "            folder_path (str): Folder path to save tokenizer checkpoints.\n",
    "            book_names (list): List of specific books to use (exclusive with max_books).\n",
    "            max_books (int): Number of books to include from clean_text (exclusive with book_names).\n",
    "            max_characters (int): Stop training after this many characters.\n",
    "            checkpoint_interval (int): Save checkpoint every N merges.\n",
    "        \"\"\"\n",
    "        combined_text, books_used, char_accum, book_count = self._get_text_from_corpus(corpus, book_names, max_books, max_characters)\n",
    "        print(f\"Total characters used: {char_accum:,}\")\n",
    "        print(f\"Training on books: {list(books_used.keys())}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.mkdir(folder_path)\n",
    "\n",
    "    \n",
    "        self.build_vocab(\n",
    "            text=combined_text,\n",
    "            books_used=books_used,\n",
    "            folder_path=folder_path,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper class for cython tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastSubwordTokenizer(BaseTokenizer):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.core = FastSubwordTokenizerCore(self.special_tokens)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.max_merges = config.max_merges\n",
    "\n",
    "    def build_vocab(self, text: str, folder_path: str, books_used: dict = None):\n",
    "        self.metadata.setdefault(\"merge_count\", 0)\n",
    "        self.metadata[\"book_used\"] = books_used\n",
    "\n",
    "        self.core.build_vocab(\n",
    "            text=text,\n",
    "            checkpoint_interval=self.config.tokenizer_checkpoint_interval,\n",
    "            folder_path=folder_path,\n",
    "            books_used=books_used,\n",
    "            vocab_size=self.vocab_size,\n",
    "            max_merges=self.max_merges,\n",
    "            metadata=self.metadata,\n",
    "        )\n",
    "\n",
    "        # Safely access exposed Python properties\n",
    "        self.vocab = self.core.py_vocab\n",
    "        self.inv_vocab = self.core.py_inv_vocab\n",
    "        self.merges = self.core.py_merges\n",
    "        self.metadata = self.core.py_metadata\n",
    "\n",
    "        # Update config with actual vocab size\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return self.core.encode(text)\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return self.core.decode(token_ids)\n",
    "\n",
    "    def batch_encode(self, texts, num_workers=None):\n",
    "        return self.core.batch_encode(texts, num_workers)\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        super().save(filepath)\n",
    "        self.core.save(filepath)\n",
    "\n",
    "    def load(self, filepath: str):\n",
    "        super().load(filepath)\n",
    "        vocab, inv_vocab, merges, metadata = self.core.load_from_file(filepath)\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = inv_vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.merges = merges\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def save_pretrained(self, folder):\n",
    "        os.makedirs(folder, exist_ok=True)\n",
    "        self.save(os.path.join(folder, \"tokenizer.json\"))\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, config, folder: str = \"./FastSubwordTokenizer\", tokenizer: str = \"tokenizer.json\"):\n",
    "        tok = cls(config)\n",
    "        tok.load(os.path.join(folder, tokenizer))\n",
    "        return tok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cython based tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# distutils: language = c++\n",
    "# cython: boundscheck=False, wraparound=False, initializedcheck=False\n",
    "\n",
    "from libc.stdlib cimport malloc, free\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import threading\n",
    "import torch\n",
    "from collections import Counter\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "\n",
    "# Utility: Get all adjacent symbol pairs in a list\n",
    "cdef set get_pairs(list symbols):\n",
    "    cdef int i\n",
    "    cdef set result = set()\n",
    "    for i in range(len(symbols) - 1):\n",
    "        result.add((symbols[i], symbols[i + 1]))\n",
    "    return result\n",
    "\n",
    "\n",
    "cdef class FastSubwordTokenizerCore:\n",
    "    \"\"\"\n",
    "    Core class for fast BPE-based subword tokenization using Cython.\n",
    "    Handles vocabulary construction, encoding, decoding, and state persistence.\n",
    "    \"\"\"\n",
    "    cdef dict vocab              # symbol -> ID\n",
    "    cdef dict inv_vocab          # ID -> symbol\n",
    "    cdef list merges             # list of merge operations (tuples)\n",
    "    cdef dict special_tokens     # special token mappings\n",
    "    cdef dict metadata           # metadata for checkpointing and resumption\n",
    "\n",
    "    def __init__(self, special_tokens=None):\n",
    "        \"\"\"\n",
    "        Initialize tokenizer core with optional special tokens.\n",
    "        \"\"\"\n",
    "        self.vocab = {}\n",
    "        self.inv_vocab = {}\n",
    "        self.merges = []\n",
    "        self.special_tokens = special_tokens if special_tokens is not None else {}\n",
    "        self.metadata = {\n",
    "            \"merge_count\": 0,\n",
    "            \"books_used\": None,\n",
    "        }\n",
    "\n",
    "\n",
    "    def build_vocab(self, text: str, max_vocab_size=None, max_merges=None,\n",
    "                    checkpoint_interval=1000, folder_path=\"./FastSubwordTokenizer\", books_used=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Builds the BPE vocabulary from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to build vocabulary from.\n",
    "            max_vocab_size (int): Optional maximum number of vocab entries.\n",
    "            max_merges (int): Total BPE merge operations to perform.\n",
    "            checkpoint_interval (int): Save to file after this many merges.\n",
    "            folder_path (str): File to save tokenizer state.\n",
    "            book_name (str): Optional identifier for tracking which text is being processed.\n",
    "        \"\"\"\n",
    "\n",
    "        stop_flag = False\n",
    "        start_time = time.time()\n",
    "        new_symbol = \"\"\n",
    "        \n",
    "        def status_thread():\n",
    "            while not stop_flag:\n",
    "                elapsed = time.time() - start_time\n",
    "                print(\n",
    "                f\"\\rMERGES: {self.metadata['merge_count']:<10}  \"\n",
    "                f\"VOCAB SIZE: {len(self.vocab):<10}  \"\n",
    "                f\"CURRENT MERGE: {new_symbol:<15}  \"  # fixed width 30 chars\n",
    "                f\"EXEC TIME: {elapsed:>7.2f}s\",\n",
    "                end='', flush=True)\n",
    "                time.sleep(1)\n",
    "                \n",
    "        thread = threading.Thread(target=status_thread)\n",
    "        thread.daemon = True\n",
    "        thread.start()\n",
    "        \n",
    "        corpus = Counter([\" \".join(tuple(word)) + self.special_tokens[\"seperator\"] for word in text.strip().split()])\n",
    "        cdef int idx = len(self.vocab)\n",
    "        cdef int merge_count = self.metadata.get(\"merge_count\", 0)\n",
    "\n",
    "        # Initialize special and base vocabulary\n",
    "        if not self.vocab:\n",
    "            for token in self.special_tokens.values():\n",
    "                self.vocab[token] = idx\n",
    "                idx += 1\n",
    "\n",
    "        symbols = set()\n",
    "        for word in corpus:\n",
    "            symbols.update(word.split())\n",
    "        for s in symbols:\n",
    "            if s not in self.vocab:\n",
    "                self.vocab[s] = idx\n",
    "                idx += 1\n",
    "\n",
    "        # Main BPE merge loop\n",
    "        while True:\n",
    "            pairs = {}\n",
    "            for word, freq in corpus.items():\n",
    "                syms = word.split()\n",
    "                for pair in get_pairs(syms):\n",
    "                    pairs[pair] = pairs.get(pair, 0) + freq\n",
    "\n",
    "            # Filter out pairs that include any special token\n",
    "            valid_pairs = {pair: freq for pair, freq in pairs.items() \n",
    "                           if pair[0] not in self.special_tokens.values() and \n",
    "                              pair[1] not in self.special_tokens.values()}\n",
    "            if not valid_pairs:\n",
    "                break\n",
    "            best_pair, freq = max(valid_pairs.items(), key=lambda x: x[1])\n",
    "            #best_pair, freq = max(pairs.items(), key=lambda x: x[1])\n",
    "\n",
    "            if not pairs:\n",
    "                break\n",
    "            if freq == 1: \n",
    "                break\n",
    "            if max_vocab_size is not None and len(self.vocab) >= max_vocab_size:\n",
    "                break\n",
    "            if max_merges is not None and merge_count >= max_merges:\n",
    "                break\n",
    "\n",
    "            # Perform merge\n",
    "            new_symbol = \"\".join(best_pair)\n",
    "            self.vocab[new_symbol] = idx\n",
    "            self.merges.append(best_pair)\n",
    "            idx += 1\n",
    "\n",
    "            # Replace all instances of the best pair in the corpus\n",
    "            bigram = re.escape(\" \".join(best_pair))\n",
    "            pattern = re.compile(rf\"(?<!\\\\S){bigram}(?!\\\\S)\")\n",
    "            new_corpus = {}\n",
    "            \n",
    "            for word in corpus:\n",
    "                new_word = pattern.sub(new_symbol, word)\n",
    "                new_corpus[new_word] = corpus[word]\n",
    "            corpus = new_corpus\n",
    "\n",
    "            # Update metadata and checkpoint if needed\n",
    "            merge_count += 1\n",
    "            self.metadata[\"merge_count\"] = merge_count\n",
    "            self.metadata[\"book_used\"] = books_used\n",
    "\n",
    "            if merge_count % checkpoint_interval == 0:\n",
    "                tokenizer_path = os.path.join(folder_path, f\"tokenizer_{merge_count}.json\")\n",
    "                self.save(tokenizer_path)\n",
    "\n",
    "        stop_flag = True\n",
    "        thread.join(timeout=2)\n",
    "        print()\n",
    "        \n",
    "        # Finalize reverse vocab\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "        \n",
    "        final_path = os.path.join(folder_path, \"tokenizer.json\")\n",
    "        self.save(final_path)\n",
    "        print(f\"FastTokenizer saved as tokenizer.json\")\n",
    "\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        \"\"\"\n",
    "        Encodes input text into a list of token IDs using full-word BPE merge strategy.\n",
    "        Each word is split into characters with </w> added at the end.\n",
    "        \"\"\"\n",
    "        cdef list token_ids = []\n",
    "        cdef int i\n",
    "        cdef str s, new_sym\n",
    "    \n",
    "        unk_id = self.vocab.get(self.special_tokens.get(\"unk\", \"<UNK>\"), -1)\n",
    "        self.merges = [tuple(m) for m in self.merges]\n",
    "\n",
    "        cdef list words = text.strip().split()\n",
    "        for word in words:\n",
    "            if word in self.special_tokens.values():\n",
    "                symbols = [word]\n",
    "            else:\n",
    "                symbols = list(word) + [self.special_tokens[\"seperator\"]]\n",
    "    \n",
    "            # Step 2: apply BPE merges\n",
    "            protected_tokens = set(self.special_tokens.values())\n",
    "            while True:\n",
    "                pairs = get_pairs(symbols)\n",
    "                pair_to_merge = None\n",
    "            \n",
    "                for merge in self.merges:\n",
    "                    if merge in pairs:\n",
    "                        if merge[0] in protected_tokens or merge[1] in protected_tokens:\n",
    "                            continue  # Skip merging if any part is protected\n",
    "                        pair_to_merge = merge\n",
    "                        break\n",
    "                if not pair_to_merge:\n",
    "                    break\n",
    "    \n",
    "                # Perform the merge\n",
    "                new_sym = \"\".join(pair_to_merge)\n",
    "                new_symbols = []\n",
    "                i = 0\n",
    "                \n",
    "                while i < len(symbols):\n",
    "                    if i < len(symbols) - 1 and (symbols[i], symbols[i + 1]) == pair_to_merge:\n",
    "                        new_symbols.append(new_sym)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_symbols.append(symbols[i])\n",
    "                        i += 1\n",
    "                symbols = new_symbols\n",
    "    \n",
    "            # Step 3: convert symbols to token IDs\n",
    "            for s in symbols:\n",
    "                token_ids.append(self.vocab.get(s, unk_id))\n",
    "    \n",
    "        return torch.tensor(token_ids)\n",
    "\n",
    "    def batch_encode(self, texts: list[str], num_workers=None) -> list[list[int]]:\n",
    "        if num_workers is None:\n",
    "            num_workers = min(cpu_count(), len(texts))\n",
    "        with Pool(num_workers) as pool:\n",
    "            return pool.map(self.encode, texts)\n",
    "\n",
    "    def decode(self, input_ids):\n",
    "        \"\"\"\n",
    "        Decodes a sequence of token IDs back to text.\n",
    "        Args:\n",
    "            input_ids (List[int] or torch.Tensor): The sequence of token IDs.\n",
    "        Returns:\n",
    "            str: The decoded text string.\n",
    "        \"\"\"\n",
    "        cdef list symbols = []\n",
    "        cdef int i\n",
    "        cdef str token\n",
    "\n",
    "        for i in input_ids:\n",
    "            token = self.inv_vocab.get(i, self.special_tokens.get(\"unk\", \"<UNK>\"))\n",
    "            symbols.append(token)\n",
    "      \n",
    "        return \"\".join(symbols).replace(self.special_tokens['seperator'], \" \")\n",
    "\n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"\n",
    "        Saves the tokenizer state (vocab, merges, metadata) to a JSON file.\n",
    "        Args:\n",
    "            filepath (str): File path to save to.\n",
    "        \"\"\"\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\n",
    "                \"vocab\": self.vocab,\n",
    "                \"merges\": self.merges,\n",
    "                \"special_tokens\": self.special_tokens,\n",
    "                \"metadata\": self.metadata\n",
    "            }, f, indent=2)\n",
    "\n",
    "    def load(self, vocab, inv_vocab, merges, metadata):\n",
    "        \"\"\"\n",
    "        Loads tokenizer state from existing Python data structures.\n",
    "        Args:\n",
    "            vocab (dict): Token-to-ID mapping.\n",
    "            inv_vocab (dict): ID-to-token mapping.\n",
    "            merges (list): Merge history.\n",
    "            metadata (dict): Metadata for training state.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.inv_vocab = inv_vocab\n",
    "        self.merges = merges\n",
    "        self.metadata = metadata\n",
    "\n",
    "    def load_from_file(self, filepath: str):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    \n",
    "        vocab = data[\"vocab\"]\n",
    "        merges = data[\"merges\"]\n",
    "        metadata = data.get(\"metadata\", {})\n",
    "        special_tokens = data.get(\"special_tokens\", {})\n",
    "        inv_vocab = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "        self.load(vocab, inv_vocab, merges, metadata)\n",
    "        self.special_tokens = special_tokens\n",
    "    \n",
    "        # Explicitly return these to caller\n",
    "        return vocab, inv_vocab, merges, metadata\n",
    "            \n",
    "    @property\n",
    "    def py_vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    @property\n",
    "    def py_inv_vocab(self):\n",
    "        return self.inv_vocab\n",
    "    \n",
    "    @property\n",
    "    def py_merges(self):\n",
    "        return self.merges\n",
    "    \n",
    "    @property\n",
    "    def py_metadata(self):\n",
    "        return self.metadata\n",
    "    \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        return self.vocab\n",
    "    \n",
    "    @property\n",
    "    def inv_vocab(self):\n",
    "        return self.inv_vocab\n",
    "    \n",
    "    @property\n",
    "    def merges(self):\n",
    "        return self.merges\n",
    "    \n",
    "    @property\n",
    "    def metadata(self):\n",
    "        return self.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKlzO1cnk7g_"
   },
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for GPT/ASR model hyperparameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=None,\n",
    "        n_ctx=448,\n",
    "        n_embd=256,\n",
    "        n_layer=4,\n",
    "        n_head=8,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        enc_dropout=0.1,\n",
    "        dec_dropout=0.1,\n",
    "        dropout=0.1,\n",
    "        self_attn_dropout=0.1,\n",
    "        lr=1e-4,\n",
    "        clip_grad_norm=1.0,\n",
    "        max_merges=None,\n",
    "        tokenizer_checkpoint_interval=1000,\n",
    "        use_cache=True,\n",
    "        gradient_checkpointing=False,\n",
    "        bos_token_id=None,\n",
    "        eos_token_id=None,\n",
    "        pad_token_id=None,\n",
    "        scheduler_type='cosine',\n",
    "        warmup_ratio=0.1,\n",
    "        early_stop_patience=3,\n",
    "        use_specaugment=True,\n",
    "        sample_rate=16000,\n",
    "    ):\n",
    "        self.dataset_root = \"/mnt/E/___COFFIN___/ASR/_datasets_/nptel-pure\"\n",
    "        self.wav_dir = os.path.join(self.dataset_root, \"wav\")\n",
    "        self.txt_dir = os.path.join(self.dataset_root, \"original_txt\")\n",
    "        self.tokenizer_dir = \"./FastSubwordTokenizer\"\n",
    "        self.tokenizer_file = \"tokenizer.json\"\n",
    "        self.best_model_path = \"./best_model.pt\"\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_ctx = n_ctx\n",
    "        self.n_embd = n_embd\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.layer_norm_epsilon = layer_norm_epsilon\n",
    "        self.enc_dropout = enc_dropout\n",
    "        self.dec_dropout = dec_dropout\n",
    "        self.dropout = dropout  # ‚Üê fixed tuple issue\n",
    "        self.self_attn_dropout = self_attn_dropout\n",
    "        self.lr = lr\n",
    "        self.clip_grad_norm = clip_grad_norm\n",
    "        self.max_merges = max_merges\n",
    "        self.tokenizer_checkpoint_interval = tokenizer_checkpoint_interval\n",
    "        self.use_cache = use_cache\n",
    "        self.gradient_checkpointing = gradient_checkpointing\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.scheduler_type = scheduler_type\n",
    "        self.warmup_ratio = warmup_ratio\n",
    "        self.early_stop_patience = early_stop_patience\n",
    "        self.use_specaugment = use_specaugment\n",
    "        self.sample_rate = sample_rate\n",
    "        self.special_tokens = {\n",
    "            \"pad\": \"<PAD>\",\n",
    "            \"unk\": \"<UNK>\",\n",
    "            \"bos\": \"<BOS>\",\n",
    "            \"eos\": \"<EOS>\",\n",
    "            \"seperator\": \"</W>\"\n",
    "        }\n",
    "\n",
    "    def set_vocab_size(self, vocab_size: int):\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def set_bos_token_id(self, tokenID: int):\n",
    "        self.bos_token_id = tokenID\n",
    "\n",
    "    def set_eos_token_id(self, tokenID: int):\n",
    "        self.eos_token_id = tokenID\n",
    "\n",
    "    def set_pad_token_id(self, tokenID: int):\n",
    "        self.pad_token_id = tokenID\n",
    "\n",
    "    def to_dict(self):\n",
    "        return self.__dict__.copy()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<ModelConfig n_layer={self.n_layer}, n_head={self.n_head}, vocab_size={self.vocab_size}, n_ctx={self.n_ctx}>\"\n",
    "\n",
    "    def save_json(self, path):\n",
    "        import json\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(self.to_dict(), f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def from_json(cls, path):\n",
    "        import json\n",
    "        with open(path, 'r') as f:\n",
    "            return cls(**json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, dataloader, tokenizer, num_samples=5):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    pad_id = tokenizer.vocab[tokenizer.special_tokens[\"pad\"]]\n",
    "    eos_id = tokenizer.vocab[tokenizer.special_tokens[\"eos\"]]\n",
    "    bos_id = tokenizer.vocab[tokenizer.special_tokens[\"bos\"]]\n",
    "\n",
    "    for i, (mels, tokens, _, _) in enumerate(dataloader):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "\n",
    "        mel = mels[0].unsqueeze(0).to(device)\n",
    "        tokens = tokens[0].to(device)\n",
    "\n",
    "        raw_ids = tokens.tolist()\n",
    "        cleaned_ids = [id for id in raw_ids if id not in [pad_id, eos_id, bos_id]]\n",
    "        true_text = tokenizer.decode(cleaned_ids)\n",
    "        pred_text = beam_search_decode(model, mel, tokenizer)\n",
    "        pred_text = tokenizer.remove_special_tokens(pred_text)\n",
    "        print(f\"\\nSample {i+1}\")\n",
    "        print(f\"Ground Truth: {true_text}\")\n",
    "        print(f\"Predicted    : {pred_text}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_search_decode(model, mel_input, tokenizer, beam_width=5, max_len=20):\n",
    "    device = mel_input.device\n",
    "    bos_token = tokenizer.vocab[tokenizer.special_tokens[\"bos\"]]\n",
    "    eos_token = tokenizer.vocab[tokenizer.special_tokens[\"eos\"]]\n",
    "\n",
    "    sequences = [[torch.tensor([bos_token], device=device), 0.0]]\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        all_candidates = []\n",
    "        for seq, score in sequences:\n",
    "            logits = model(mel_input, seq.unsqueeze(0))  # (1, T, V)\n",
    "            probs = F.log_softmax(logits[:, -1, :], dim=-1)  # (1, V)\n",
    "            topk_probs, topk_ids = torch.topk(probs, beam_width)\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                token = topk_ids[0, i].item()\n",
    "                new_seq = torch.cat([seq, torch.tensor([token], device=device)])\n",
    "                new_score = score + topk_probs[0, i].item()\n",
    "                all_candidates.append((new_seq, new_score))\n",
    "\n",
    "        sequences = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "        if all(seq[0][-1].item() == eos_token for seq in sequences):\n",
    "            break\n",
    "\n",
    "    return tokenizer.decode(sequences[0][0].tolist())\n",
    "\n",
    "\n",
    "def train_for_epochs(model, dataloader, optimizer, tokenizer, num_epochs):\n",
    "    pad_id = tokenizer.vocab[tokenizer.special_tokens[\"pad\"]]\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTfModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Initialize token embeddings\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "\n",
    "        # Initialize LM head\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Tie weights initially\n",
    "        self.tie_weights()\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n",
    "\n",
    "    def set_input_embeddings(self, new_emb):\n",
    "        self.embed_tokens = new_emb\n",
    "        self.tie_weights()\n",
    "\n",
    "    def resize_token_embeddings(self, new_vocab_size):\n",
    "        old_embedding = self.embed_tokens\n",
    "        self.embed_tokens = nn.Embedding(new_vocab_size, old_embedding.embedding_dim)\n",
    "        with torch.no_grad():\n",
    "            self.embed_tokens.weight[:old_embedding.num_embeddings] = old_embedding.weight\n",
    "        self.config.vocab_size = new_vocab_size\n",
    "        self.tie_weights()\n",
    "\n",
    "    def tie_weights(self):\n",
    "        if hasattr(self, \"lm_head\") and hasattr(self, \"embed_tokens\"):\n",
    "            self.lm_head.weight = self.embed_tokens.weight\n",
    "\n",
    "    def save_pretrained(self, save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        torch.save(self.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "        with open(os.path.join(save_dir, \"config.json\"), \"w\") as f:\n",
    "            json.dump(self.config.__dict__, f, indent=2)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, load_dir: str, model_name: str=None):\n",
    "        with open(os.path.join(load_dir, \"config.json\")) as f:\n",
    "            config_dict = json.load(f)\n",
    "        config = GPT2Config(**config_dict)\n",
    "        model = cls(config)\n",
    "        if model_name is None:\n",
    "            model_name = \"pytorch_model.bin\"\n",
    "        model.load_state_dict(torch.load(os.path.join(load_dir, model_name), map_location=\"cpu\"))\n",
    "        return model\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, input_ids, max_new_tokens=20):\n",
    "        self.eval()\n",
    "        for _ in tqdm(range(max_new_tokens), desc=\"Generating\"):\n",
    "            logits = self.forward(input_ids)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True).data[0]\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "        return input_ids\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_loss(self, dataloader):\n",
    "        self.eval()\n",
    "        total_loss = 0\n",
    "        for x, y in dataloader:\n",
    "            x, y = x.to(self.model_device), y.to(self.model_device)\n",
    "            logits = self(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n",
    "        return {\"input_ids\": input_ids}\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_validation(self, dataloader, tokenizer, criterion, num_samples=2):\n",
    "        self.eval()\n",
    "        device = self.model_device\n",
    "        total_loss = 0\n",
    "    \n",
    "        pad_id = tokenizer.vocab[tokenizer.special_tokens[\"pad\"]]\n",
    "        eos_id = tokenizer.vocab[tokenizer.special_tokens[\"eos\"]]\n",
    "        bos_id = tokenizer.vocab[tokenizer.special_tokens[\"bos\"]]\n",
    "    \n",
    "        for i, (mels, tokens, _, _) in enumerate(dataloader):\n",
    "            mels, tokens = mels.to(device), tokens.to(device)\n",
    "            logits = self(mels, tokens[:, :-1])\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tokens[:, 1:].reshape(-1))\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "            if i < num_samples:\n",
    "                raw_ids = tokens[0].tolist()\n",
    "                cleaned_ids = [id for id in raw_ids if id not in [pad_id, eos_id, bos_id]]\n",
    "                true_text = tokenizer.decode(cleaned_ids)\n",
    "                pred_text = self.beam_search(mels[0].unsqueeze(0), tokenizer)\n",
    "                print(f\"\\nSample {i+1}\")\n",
    "                print(f\"Ground Truth: {true_text}\")\n",
    "                print(f\"Predicted    : {pred_text}\")\n",
    "    \n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return str(next(self.parameters()).device)\n",
    "\n",
    "    @property\n",
    "    def supports_gradient_checkpointing(self):\n",
    "        return getattr(self.config, \"gradient_checkpointing\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.n_head = config.n_head\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        self.q_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.k_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.v_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "        self.out_proj = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        B, T, C = x.size()\n",
    "        context = x if context is None else context\n",
    "\n",
    "        q = self.q_proj(x).view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = self.k_proj(context).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(context).view(B, -1, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        att = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        if mask is not None:\n",
    "            att = att.masked_fill(mask == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        out = att @ v\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(config.n_embd, 4 * config.n_embd, bias=False)\n",
    "        self.fc2 = nn.Linear(4 * config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.gelu(self.fc1(x)))\n",
    "\n",
    "\n",
    "class WhisperEncoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class WhisperDecoderBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.self_attn = MultiHeadAttention(config)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.cross_attn = MultiHeadAttention(config)\n",
    "        self.ln3 = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x, encoder_output, self_mask=None):\n",
    "        x = x + self.self_attn(self.ln1(x), mask=self_mask)\n",
    "        x = x + self.cross_attn(self.ln2(x), context=encoder_output)\n",
    "        x = x + self.mlp(self.ln3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class WhisperModel(BaseTfModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_mel_proj = nn.Linear(80, config.n_embd)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            WhisperEncoderBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "\n",
    "        # Decoder\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.n_ctx, config.n_embd))\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            WhisperDecoderBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "\n",
    "        # Final\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        self._tie_weights()\n",
    "\n",
    "    def _tie_weights(self):\n",
    "        self.lm_head.weight = self.token_emb.weight\n",
    "\n",
    "    def forward(self, mel_input, tgt_ids):\n",
    "        device = mel_input.device\n",
    "        B, T = tgt_ids.size()\n",
    "\n",
    "        max_T = self.pos_emb.shape[1]\n",
    "        if T > max_T:\n",
    "            tgt_ids = tgt_ids[:, :max_T]\n",
    "            T = max_T\n",
    "\n",
    "        # Encode\n",
    "        x = self.encoder_mel_proj(mel_input)\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x)\n",
    "        encoder_out = x\n",
    "\n",
    "        # Decode\n",
    "        tok_embed = self.token_emb(tgt_ids.long()) + self.pos_emb[:, :T, :]\n",
    "        mask = torch.tril(torch.ones(T, T, device=device)).bool()\n",
    "        x = tok_embed\n",
    "\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, encoder_out, self_mask=mask)\n",
    "\n",
    "        x = self.ln_f(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "    def generate(self, mel_input, tokenizer, max_new_tokens=100):\n",
    "        device = mel_input.device\n",
    "        bos_token_id = tokenizer.vocab[tokenizer.special_tokens[\"bos\"]]\n",
    "        eos_token_id = tokenizer.vocab[tokenizer.special_tokens[\"eos\"]]\n",
    "\n",
    "        input_ids = torch.tensor([[bos_token_id]], device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Encode once\n",
    "            x = self.encoder_mel_proj(mel_input)\n",
    "            for layer in self.encoder_layers:\n",
    "                x = layer(x)\n",
    "            encoder_out = x\n",
    "\n",
    "            for _ in range(max_new_tokens):\n",
    "                tok_embed = self.token_emb(input_ids) + self.pos_emb[:, :input_ids.size(1), :]\n",
    "                mask = torch.tril(torch.ones(input_ids.size(1), input_ids.size(1), device=device)).bool()\n",
    "                x = tok_embed\n",
    "\n",
    "                for layer in self.decoder_layers:\n",
    "                    x = layer(x, encoder_out, self_mask=mask)\n",
    "\n",
    "                logits = self.lm_head(self.ln_f(x))\n",
    "                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "                input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "                if next_token.item() == eos_token_id:\n",
    "                    break\n",
    "\n",
    "        return tokenizer.decode(input_ids[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, root_dir, tokenizer, config, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Path to root of dataset (e.g., 'nptel-pure')\n",
    "            tokenizer (Tokenizer): Your tokenizer instance\n",
    "            config (ModelConfig): Must have sample_rate, n_ctx, use_specaugment\n",
    "            split (str): \"train\" or \"val\" (controls augmentation)\n",
    "        \"\"\"\n",
    "        self.wav_dir = os.path.join(root_dir, \"wav\")\n",
    "        self.txt_dir = os.path.join(root_dir, \"original_txt\")\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.sample_rate = config.sample_rate\n",
    "        self.max_len = config.n_ctx\n",
    "        self.augment = (split == \"train\" and getattr(config, \"use_specaugment\", False))\n",
    "\n",
    "        self.items = sorted([\n",
    "            f for f in os.listdir(self.wav_dir)\n",
    "            if f.endswith(\".wav\") and os.path.exists(os.path.join(self.txt_dir, f.replace(\".wav\", \".txt\")))\n",
    "        ])\n",
    "\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=400,\n",
    "            win_length=400,\n",
    "            hop_length=160,\n",
    "            n_mels=80\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def truncate_to_eos(token_ids: torch.Tensor, eos_id: int):\n",
    "        eos_positions = (token_ids == eos_id).nonzero(as_tuple=True)[0]\n",
    "        if len(eos_positions) > 0:\n",
    "            token_ids = token_ids[:eos_positions[0] + 1]\n",
    "        return token_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wav_file = self.items[idx]\n",
    "        txt_file = wav_file.replace(\".wav\", \".txt\")\n",
    "\n",
    "        # ‚úÖ Load and resample audio\n",
    "        waveform, sr = safe_load(os.path.join(self.wav_dir, wav_file))\n",
    "        if sr != self.sample_rate:\n",
    "            resample = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)\n",
    "            waveform = resample(waveform)\n",
    "\n",
    "        # ‚úÖ Add white noise if augmenting\n",
    "        if self.augment:\n",
    "            waveform = waveform + 0.002 * torch.randn_like(waveform)\n",
    "\n",
    "        # ‚úÖ Compute Mel spectrogram: (time, 80)\n",
    "        mel = self.mel_transform(waveform).squeeze(0).transpose(0, 1)\n",
    "\n",
    "        # ‚úÖ Load and tokenize text\n",
    "        with open(os.path.join(self.txt_dir, txt_file), \"r\") as f:\n",
    "            text = f.read().strip().upper()       \n",
    "            text = self.tokenizer.add_special_tokens(text)\n",
    "        token_tensor = self.tokenizer.encode(text)\n",
    "        \n",
    "        # eos_id = self.tokenizer.vocab[self.tokenizer.special_tokens[\"eos\"]]\n",
    "        # token_tensor = self.truncate_to_eos(token_tensor, eos_id)\n",
    "        # token_tensor = self.tokenizer.pad(token_tensor.tolist())[:self.max_len]\n",
    "        # token_tensor = torch.tensor(token_tensor, dtype=torch.long)\n",
    "\n",
    "        return mel, token_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(tokenizer):\n",
    "    def collate_fn(batch):\n",
    "        mels, tokens = zip(*batch)\n",
    "        mel_lens = [mel.shape[0] for mel in mels]\n",
    "        max_mel_len = max(mel_lens)\n",
    "        padded_mels = torch.stack([\n",
    "            F.pad(mel, (0, 0, 0, max_mel_len - mel.shape[0])) for mel in mels\n",
    "        ])\n",
    "        pad_id = tokenizer.vocab[tokenizer.special_tokens[\"pad\"]]\n",
    "        padded_tokens = torch.nn.utils.rnn.pad_sequence(tokens, batch_first=True, padding_value=pad_id)\n",
    "        token_lens = [len(t) for t in tokens]\n",
    "        return padded_mels, padded_tokens, mel_lens, token_lens\n",
    "    return collate_fn\n",
    "    \n",
    "\n",
    "def prepare_dataloaders(dataset, batch_size=2):\n",
    "    train_size = int(0.9 * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_indices = list(range(train_size))\n",
    "    val_indices = list(range(train_size, train_size + val_size))\n",
    "\n",
    "    train_dataset = Subset(\n",
    "        ASRDataset(dataset.wav_dir.rsplit(\"/\", 1)[0], dataset.tokenizer, dataset.config, split=\"train\"),\n",
    "        train_indices\n",
    "    )\n",
    "    val_dataset = Subset(\n",
    "        ASRDataset(dataset.wav_dir.rsplit(\"/\", 1)[0], dataset.tokenizer, dataset.config, split=\"val\"),\n",
    "        val_indices\n",
    "    )\n",
    "\n",
    "    collate = make_collate_fn(dataset.tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Soundfile util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_load(filepath):\n",
    "    try:\n",
    "        return torchaudio.load(filepath)\n",
    "    except RuntimeError:\n",
    "        print(f\"[Fallback] Using soundfile for: {filepath}\")\n",
    "        data, sr = sf.read(filepath)\n",
    "        return torch.from_numpy(data).unsqueeze(0).float(), sr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhisperPreprocessor:\n",
    "    def __init__(self, config):\n",
    "        self.sample_rate = config.sample_rate\n",
    "        self.n_mels = config.n_mels if hasattr(config, \"n_mels\") else 80\n",
    "        self.n_fft = config.n_fft if hasattr(config, \"n_fft\") else 400\n",
    "        self.hop_length = config.hop_length if hasattr(config, \"hop_length\") else 160\n",
    "        self.win_length = config.win_length if hasattr(config, \"win_length\") else 400\n",
    "\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate,\n",
    "            n_fft=self.n_fft,\n",
    "            win_length=self.win_length,\n",
    "            hop_length=self.hop_length,\n",
    "            n_mels=self.n_mels,\n",
    "            center=True,\n",
    "            power=2.0,\n",
    "            normalized=False,\n",
    "        )\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(top_db=80)\n",
    "\n",
    "    def __call__(self, waveform: torch.Tensor) -> torch.Tensor:\n",
    "        if waveform.dim() == 1:\n",
    "            waveform = waveform.unsqueeze(0)\n",
    "        mel = self.mel_spec(waveform)\n",
    "        mel_db = self.db_transform(mel)\n",
    "        return mel_db.squeeze(0).transpose(0, 1)  # [T, 80] for compatibility\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_tokenizer = FastSubwordTokenizer.from_pretrained(\n",
    "    config, \n",
    "    folder=\"/mnt/E/___COFFIN___/ASR/FastSubwordTokenizer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_text = \"\"\n",
    "\n",
    "# for file in sorted(os.listdir(config.txt_dir)):\n",
    "#     with open(os.path.join(config.txt_dir, file), \"r\") as f:\n",
    "#         corpus_text += config.special_tokens['bos'] + f.read().strip().upper() + \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast_tokenizer.build_vocab(text=corpus_text, folder_path=config.tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.set_vocab_size(fast_tokenizer.get_vocab_size())\n",
    "config.set_pad_token_id(fast_tokenizer.vocab[fast_tokenizer.special_tokens[\"pad\"]])\n",
    "config.set_bos_token_id(fast_tokenizer.vocab[fast_tokenizer.special_tokens[\"bos\"]])\n",
    "config.set_eos_token_id(fast_tokenizer.vocab[fast_tokenizer.special_tokens[\"eos\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate and test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-AoFnP2m13XU"
   },
   "outputs": [],
   "source": [
    "model = WhisperModel(config).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.lr, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = WhisperPreprocessor(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sr = torchaudio.load(\"/mnt/E/___COFFIN___/ASR/_datasets_/nptel-pure/wav/000a01ea126c4b59aa992d992a1a8e1076a53bf7755eb6a0f80a2cdb.wav\")\n",
    "if sr != config.sample_rate:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=config.sample_rate)(waveform)\n",
    "\n",
    "mel = preprocessor(waveform).unsqueeze(0).to(model.device)  # [1, T, 80]\n",
    "\n",
    "# Generate tokens\n",
    "generated = model.generate(mel_input=mel, tokenizer=fast_tokenizer, max_new_tokens=50)\n",
    "predicted_text = fast_tokenizer.remove_special_tokens(generated)\n",
    "\n",
    "# Load ground truth\n",
    "txt_path = os.path.join(\"/mnt/E/___COFFIN___/ASR/_datasets_/nptel-pure/original_txt\",\"000a01ea126c4b59aa992d992a1a8e1076a53bf7755eb6a0f80a2cdb.txt\")\n",
    "with open(txt_path, \"r\") as f:\n",
    "    ground_truth = f.read().strip().upper()\n",
    "\n",
    "# Show results\n",
    "print(\"\\nGround Truth:\")\n",
    "print(ground_truth)\n",
    "print(\"\\nPredicted:   \")\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ASRDataset(config.dataset_root, fast_tokenizer, config, split=\"train\")\n",
    "train_loader, val_loader = prepare_dataloaders(dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel, tokens = dataset[0]\n",
    "print(f\"Mel shape    : {mel.shape}\")      # Expected: [T, 80]\n",
    "print(f\"Token shape  : {tokens.shape}\")   # Expected: [<= config.n_ctx]\n",
    "print(f\"Token IDs    : {tokens}\")    # First 10 token IDs\n",
    "\n",
    "decode = fast_tokenizer.decode(tokens)\n",
    "print(decode)\n",
    "print()\n",
    "print(fast_tokenizer.remove_special_tokens(decode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch from train loader\n",
    "for mels, tokens, mel_lens, token_lens in train_loader:\n",
    "    print(\"Mel shape:   \", mels.shape)   # [B, T, 80]\n",
    "    print(\"Tokens shape:\", tokens.shape) # [B, L]\n",
    "\n",
    "    input_ids = tokens[:, :-1]\n",
    "    target_ids = tokens[:, 1:]\n",
    "\n",
    "    print(\"Input IDs shape:  \", input_ids.shape)\n",
    "    print(\"Target IDs shape: \", target_ids.shape)\n",
    "\n",
    "    # Decode input and target for sample 0\n",
    "    print(\"\\nDecoded Input IDs:\")\n",
    "    print(fast_tokenizer.decode(input_ids[0].tolist()))\n",
    "    print(\"\\nDecoded Target IDs:\")\n",
    "    print(fast_tokenizer.decode(target_ids[0].tolist()))\n",
    "\n",
    "    break  # Only one batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAPFNg-5mnWK"
   },
   "outputs": [],
   "source": [
    "def train_for_epochs_with_val(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    tokenizer,\n",
    "    num_epochs=12,\n",
    "    clip=1.0,\n",
    "    early_stop_patience=3\n",
    "):\n",
    "    pad_id = tokenizer.vocab[tokenizer.special_tokens[\"pad\"]]\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=0.1)\n",
    "    device = next(model.parameters()).device\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        start = time.time()\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}\", dynamic_ncols=True)\n",
    "        for mels, tokens, _, _ in pbar:\n",
    "            mels, tokens = mels.to(device), tokens.to(device)\n",
    "            input_ids = tokens[:, :-1]\n",
    "            target_ids = tokens[:, 1:]\n",
    "\n",
    "            with autocast(device_type=model.device):\n",
    "                logits = model(mels, input_ids)\n",
    "\n",
    "                # Match target length with logits\n",
    "                logits_len = logits.size(1)\n",
    "                target_ids = target_ids[:, :logits_len]\n",
    "\n",
    "                loss = loss_fn(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1).long())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for mels, tokens, _, _ in val_loader:\n",
    "                mels, tokens = mels.to(device), tokens.to(device)\n",
    "                input_ids = tokens[:, :-1]\n",
    "                target_ids = tokens[:, 1:]\n",
    "\n",
    "                logits = model(mels, input_ids)\n",
    "\n",
    "                # Match target length with logits\n",
    "                logits_len = logits.size(1)\n",
    "                target_ids = target_ids[:, :logits_len]\n",
    "\n",
    "                val_loss = loss_fn(logits.reshape(-1, logits.size(-1)), target_ids.reshape(-1).long())\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Time: {time.time() - start:.2f}s\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            print(\"Saved best model\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stop_patience:\n",
    "                print(f\"Early stopping at epoch {epoch} (no improvement for {early_stop_patience} epochs)\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f8_JP1lE8C2x",
    "outputId": "0fb3b4d2-b8d8-4867-eff7-30ea6f3ba2fb"
   },
   "outputs": [],
   "source": [
    "train_for_epochs_with_val(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    tokenizer=fast_tokenizer,\n",
    "    num_epochs=100,\n",
    "    clip=1.0,\n",
    "    early_stop_patience=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmMQSdtUXcEg",
    "outputId": "3199841b-96bf-45a8-babf-304ca4642f77"
   },
   "outputs": [],
   "source": [
    "waveform, sr = torchaudio.load(\"/mnt/E/___COFFIN___/ASR/_datasets_/nptel-pure/wav/000a01ea126c4b59aa992d992a1a8e1076a53bf7755eb6a0f80a2cdb.wav\")\n",
    "if sr != config.sample_rate:\n",
    "    waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=config.sample_rate)(waveform)\n",
    "\n",
    "mel = preprocessor(waveform).unsqueeze(0).to(model.device)  # [1, T, 80]\n",
    "\n",
    "# Generate tokens\n",
    "generated = model.generate(mel_input=mel, tokenizer=fast_tokenizer, max_new_tokens=50)\n",
    "predicted_text = fast_tokenizer.remove_special_tokens(generated)\n",
    "\n",
    "# Load ground truth\n",
    "txt_path = os.path.join(\"/mnt/E/___COFFIN___/ASR/_datasets_/nptel-pure/original_txt\",\"000a01ea126c4b59aa992d992a1a8e1076a53bf7755eb6a0f80a2cdb.txt\")\n",
    "with open(txt_path, \"r\") as f:\n",
    "    ground_truth = f.read().strip().upper()\n",
    "\n",
    "# Show results\n",
    "print(\"\\nGround Truth:\")\n",
    "print(ground_truth)\n",
    "print(\"\\nPredicted:   \")\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQ4n6zL4otk_",
    "outputId": "898569d4-3e52-4a84-a4d6-f35e495d9579"
   },
   "outputs": [],
   "source": [
    "text = \"I AM FINE\"\n",
    "print(\"Encoded:\", fast_tokenizer.encode(text))\n",
    "print(\"Decoded:\", fast_tokenizer.decode(fast_tokenizer.encode(text).tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
